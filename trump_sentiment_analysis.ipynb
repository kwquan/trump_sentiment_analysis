{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from dateutil import parser\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import tweepy\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change twitterdb to name of your database\n",
    "MONGO_HOST= 'mongodb://localhost/twitterdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that connects to MongoDB & converts text column to dataframe\n",
    "def connect():\n",
    "    client = MongoClient(MONGO_HOST)\n",
    "    db = client.twitterdb\n",
    "    tweets = db.tweets\n",
    "    df = pd.DataFrame(list(tweets.find({}, { '_id': 0,'text': 1})))\n",
    "    return df \n",
    "\n",
    "#Function for cleaning tweets\n",
    "def clean_tweets(inp_df):\n",
    "    stopword_list = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    inp_df['clean_tweets'] = None\n",
    "    for i in range(0,len(inp_df['text'])):\n",
    "        exclude = ['[^a-zA-Z]','rt','RT','http\\S+']\n",
    "        exclusions = '|'.join(exclude)\n",
    "        text = re.sub(exclusions, ' ',str(inp_df['text'][i]))\n",
    "        text = text.lower()\n",
    "        words = text.split()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words if word not in stopword_list]\n",
    "        inp_df['clean_tweets'][i] = ' '.join(words)\n",
    "    return inp_df    \n",
    "\n",
    "#Function for counting sentiment value for each tweet\n",
    "def sentiment(tweet):\n",
    "    analysis = TextBlob(tweet)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return 1\n",
    "    elif analysis.sentiment.polarity == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1     \n",
    "    \n",
    "#Function for separating tweets\n",
    "def separate(inp_df):\n",
    "    pos_tweets = [tweet for index,tweet in enumerate(inp_df['Sentiment']) if inp_df['Sentiment'][index] > 0]\n",
    "    neu_tweets = [tweet for index,tweet in enumerate(inp_df['Sentiment']) if inp_df['Sentiment'][index] == 0]\n",
    "    neg_tweets = [tweet for index,tweet in enumerate(inp_df['Sentiment']) if inp_df['Sentiment'][index] < 0]\n",
    "    return pos_tweets,neu_tweets,neg_tweets\n",
    "\n",
    "#Function for creating wordcloud\n",
    "def word_cloud(inp_df):\n",
    "    plt.subplots(figsize = (12,10))\n",
    "    wordcloud = WordCloud(background_color = 'white',width = 1000,height = 800).generate(\" \".join(inp_df['clean_tweets']))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Scraps tweets of donald trump & prints sentiment percentage & wordcloud for every 1000 tweets\n",
    "class Streamlistener(tweepy.StreamListener):\n",
    "    def on_connect(self):\n",
    "        print(\"You are connected to the Twitter API\")\n",
    "        \n",
    "    def on_error(self):\n",
    "        if error_code != 200:\n",
    "            print('error')\n",
    "            return False\n",
    "        \n",
    "    def on_data(self,data):\n",
    "        try:\n",
    "            client = MongoClient(MONGO_HOST)\n",
    "            db = client.twitterdb\n",
    "            datajson = json.loads(data)\n",
    "            db.tweets.insert(datajson)\n",
    "            created_at = parser.parse(datajson['created_at'])         \n",
    "            print(\"Tweet collected at: {}\".format(str(created_at)))\n",
    "            if db.tweets.find().count()%1000 == 0:\n",
    "                df = connect()\n",
    "                df = clean_tweets(df)\n",
    "                df['Sentiment'] = np.array([sentiment(x) for x in df['clean_tweets']])\n",
    "                pos_tweets,neu_tweets,neg_tweets = separate(df)\n",
    "                print(\"Positive tweets percentage: {}%\".format(round(100*(len(pos_tweets)/len(df['clean_tweets'])),2)))\n",
    "                print(\"Negative tweets percentage: {}%\".format(round(100*(len(neg_tweets)/len(df['clean_tweets'])),2)))\n",
    "                print(\"Neutral tweets percentage: {}%\".format(round(100*(len(neu_tweets)/len(df['clean_tweets'])),2)))\n",
    "                word_cloud(df)\n",
    "                time.sleep(5)\n",
    "\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    count = 0\n",
    "    consumer_key = ''\n",
    "    consumer_secret = ''\n",
    "    access_key = ''\n",
    "    access_secret = ''\n",
    "\n",
    "    auth = tweepy.OAuthHandler(consumer_key,consumer_secret)\n",
    "    auth.set_access_token(access_key,access_secret)\n",
    "    api =tweepy.API(auth, wait_on_rate_limit=True)\n",
    "    listener = Streamlistener(api=api)\n",
    "    stream = tweepy.Stream(auth,listener=listener)\n",
    "    track = ['trump','donald trump']\n",
    "    stream.filter(track = track, languages = ['en'])      "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
